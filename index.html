<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Deformable Neural Radiance Fields creates free-viewpoint portraits (nerfies) from casually captured videos.">
  <meta name="keywords" content="Nerfies, D-NeRF, NeRF">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <title>Task-Aware Semantic Map++</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="stylesheet" href="./static/css/drop_down.css">
  <link rel="icon" href="./static/images/rwh.png">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>

  <!-- Three.js ê´€ë ¨ ìŠ¤í¬ë¦½íŠ¸ ì¶”ê°€ -->
  <script src="https://cdnjs.cloudflare.com/ajax/libs/three.js/r128/three.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/three@0.128.0/examples/js/loaders/PLYLoader.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/three@0.128.0/examples/js/controls/OrbitControls.js"></script>

</head>
<body>

<nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
  <div class="navbar-menu">
    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
      <a class="navbar-item" href="https://sites.google.com/view/robots-oh">
      <span class="icon">
          <i class="fas fa-home"></i>
      </span>
      </a>

      <div class="navbar-item has-dropdown is-hoverable">
        <a class="navbar-link">
          Related Research
        </a>
        <div class="navbar-dropdown">
          <a class="navbar-item" href="https://robots-oh.github.io/TASMap/">
            TASMap
          </a>
        </div>
      </div>
    </div>

  </div>
</nav>


<section class="hero top-banner">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">Task-Aware Semantic Map<span class="plus-bounce">++</span>: Cost-Efficient Task Assignment with Advanced Benchmark</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://sites.google.com/view/robots-oh/members#h.2cy2g7krx7mb">Daewon Choi</a><sup>*</sup>,</span>
            <span class="author-block">
              <a href="https://sites.google.com/view/robots-oh/members#h.oc8l44mgli6o">Soeun Hwang</a><sup>*</sup>,</span>
            </span>
            <span class="author-block">
              <a href="https://sites.google.com/view/robots-oh/yoonseon-oh">Yoonseon Oh</a>
            </span>
            <span class="author-block">
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block">All authors are with Hanyang University.</span>
          </div>

          <div class="equal_contribution">
            <span class="author-block"><sup>*</sup>Equal contribution</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <span class="link-block">
                <!-- <a href="https://ieeexplore.ieee.org/abstract/document/11127372?casa_token=uFsS8zDCiT4AAAAA:hPqJMA-SqTv0chUrchHXOMY55we-F50XtNFIHZdlrH55J3aiKA2Vnm1wvnNaFzTPpYBi-8XXEx8"  -->
                 <a href ="https://ieeexplore.ieee.org/abstract/document/11361083"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <!-- Video Link. -->
              <span class="link-block">
                <!-- <a href="https://www.youtube.com/watch?v=GLPQwCKYK9A" -->
                 <a href ="#" onclick="alert('Coming soon ðŸ˜…');"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-youtube"></i>
                  </span>
                  <span>Video</span>
                </a>
              </span>
              <!-- Dataset Link. -->
              <span class="link-block">
                <a href="https://github.com/robots-oh/tasmap-plus-plus?tab=readme-ov-file#data-collection"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="far fa-images"></i>
                  </span>
                  <span>Data</span>
                </a>
              </span>

              <span class="link-block">
                <a href="https://github.com/robots-oh/tasmap-plus-plus"
                class="external-link button is-normal is-rounded is-dark">
                <span class="icon">
                    <i class="fab fa-github"></i>
                </span>
                <span>Code</span>
                </a>
            </span>

            </div>
            
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section">
  <!-- Paper video. -->
  <div class="container is-max-desktop main-content">
    <div class="columns is-centered">
      <div class="column is-full-width">
        <div id="icra_comment">
          ðŸŽ‰Congratulation!ðŸŽ‰ <br> Our paper got accepted in IEEE Robotics and Automation Letters (RA-Letters) ðŸ¤­
        </div>
        <div class="publication-video">
          <!-- <iframe src="https://www.youtube.com/embed/eNEaDmD4cDk?si=Sv6Wvye92bTUGOAI"
                  frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe> -->
          <video controls poster="static/images/thumbnail.png">
            <source src="https://github.com/robots-oh/tasmap_pp/raw/refs/heads/main/static/videos/TASMap++_video_new_compressed.mp4" type="video/mp4">
          </video>
        </div>
        <p>
          This video is attached as a supplementary material for RA-Letters.
        </p>
      </div>
    </div>
  </div>
</section>
  
<section class="abstract_section">
  <!-- Abstract. -->
    <div class="container is-max-desktop main-content">
      <div class="columns is-centered">
        <div class="column is-full-width">
          <h2 class="title is-3 title-centered">Abstract</h2>
          <div class="container">
            <div class="image-center">
              <img src="./static/images/abstract.jpg"
              alt="Task-Aware Semantic Map++ Representative Figure"
              class="representative-image">
            </div>
            <p class="caption">
              Enabling robots to perform diverse tasks autonomously requires a sophisticated semantic understanding of 3D scenes. However, conventional scene representations, which primarily rely on static attributes like visual information or object labels, have significant limitations in allowing robots to infer context-aware actions. We introduce <strong>Task-Aware Semantic Map++</strong>  (TASMap++), the framework that overcomes these limitations by constructing a map that assigns appropriate tasks to objects based on their holistic context.  While prior work like TASMap pioneered this task-centric approach, it suffered from high computational costs and inaccuracies due to its reliance on single-frame analysis, which often fails to capture an object's complete state. In contrast, TASMap++ resolves these issues with a multi-view synthesis pipeline that integrates multiple perspectives of an object for task assignment, resulting in significantly improved computational efficiency over its predecessor. Furthermore, to overcome biases in the existing TASMap evaluation, we established a reliable benchmark derived from the consensus of 32 participants across 231 cluttered scenes. On this benchmark, TASMap++ demonstrates superior accuracy over baselines. Finally, we introduce context-aware grounding, a paradigm distinct from conventional object grounding that relies on visual and spatial attributes. We present a downstream application of TASMap++ as a method to address this challenge and show experimentally that conventional grounding methods struggle in this setting, whereas TASMap++ is markedly more effective. To confirm these findings, the framework's robustness and practicality were validated through extensive experiments on 3D indoor datasets, including real-world scan datasets.
            </p>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section>
  <div class="container main-content">
    <div class="columns is-centered has-text-justified">
      <div class="column is-full-width">
        <h2 class="title is-3 title-centered">TASMap++ Benchmark <a href ="http://ec2-43-201-242-118.ap-northeast-2.compute.amazonaws.com/select.html?demo=true"> <i class="fas fa-external-link-alt"></i> </a> </h2> 

        <div class="content">
          <div class="image-center">
            <img src="./static/images/benchmark_analysis.jpg" alt="Benchmark Image"/>
          </div>
          <p class>
            On average, each object is assigned approximately 1.3 tasks, confirming that the benchmark annotations are both selective and plausible. Task distributions vary distinctly across different scenarios (e.g., frequent 'Relocate' in disordered scenes), aligning well with the intended semantic context of each setting. A correlation analysis reveals meaningful relationships between tasks, such as strong positive links between complementary actions like mopping and vacuuming. Frequent co-occurrences, such as folding and relocating, further demonstrate logical task groupings made by human annotators. Overall, these consistent patterns and correlations validate that the benchmark captures meaningful structural preferences for task assignment evaluation.
          </p>
        </div>

        <div class="content">
          <div class="image-center">
            <img src="./static/images/benchmark_compare.jpg" alt="Benchmark Image"/>
          </div>
          <p class>
            To quantify annotator bias, we analyzed label agreement using Jaccard and F1 scores under three settings: consensus-based TASMap++, single-annotator simulation (TASMap++ Single), and the original TASMap. The results show that TASMap and TASMap++ (Single) achieve similarly low agreement scores, which are significantly lower than those of the consensus-based TASMap++ setting. These lower scores in the single-annotator settings highlight that individual labels are inherently inconsistent and prone to specific annotator biases. This indicates that the original TASMap functions effectively as a single-annotator dataset, inheriting the instability of individual preferences. In contrast, the consensus-based TASMap++ is proven to be a more stable benchmark that consistently captures overall human preferences by mitigating individual bias.
          </p>
        </div>

      </div>
    </div>
  </div>
</section>

<section>
  <div class="container main-content">
    <div class="columns is-centered has-text-justified">
      <div class="column is-full-width">
        <h2 class="title is-3 title-centered">TASMap++ Construction</h2>
        <div class="content">
          <img src="./static/images/main_framework.jpg"/>
          <p>
            (Left) Overview of the proposed framework. Given RGB-D sequence and camera pose as input, <strong>3D Instance Segmentation</strong> module returns Object-Segmentation Entity with regarding reconstructed point clouds. Subsequently, through the <strong>Mask Refinement</strong>, <strong>View Selection</strong> and <strong>Task Assignment</strong> modules, we obtain the Object-Task Entity. Based on these two entities, TASMap++ is constructed. (Right) Process of <strong>Context-Aware Grounding</strong> used in experiments.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>

<section>
  <div class="container main-content">
    <div class="columns is-centered has-text-justified">
      <div class="column is-full-width">
        <h2 class="title is-3 title-centered">Results <a href ="http://ec2-43-201-242-118.ap-northeast-2.compute.amazonaws.com/select.html?mode=results"> <i class="fas fa-external-link-alt"></i> </a> </h2>
        <div class="content">
          <div style="text-align: center;">
            <img src="./static/images/results.jpg"/>
          </div>
          <p>
            The center two figure provide visualization of TASMap++. For each object, the assigned task, label, and a unique ID to distinguish between identical labels are displayed. The lower portion shows the objects selected in Context-Aware Object Grounding for 5 explicit (purple) and 3 implicit (blue) queries.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>



<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@article{choi2026task,
  title={Task-Aware Semantic Map++: Cost-Efficient Task Assignment with Advanced Benchmark},
  author={Choi, Daewon and Hwang, Soeun and Oh, Yoonseon},
  journal={IEEE Robotics and Automation Letters},
  year={2026},
  publisher={IEEE}
}
    </code></pre>  
  </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <video id="hello_robot" autoplay loop muted>
        <source src="https://github.com/robots-oh/TASMap/raw/refs/heads/main/static/videos/hello_robot.mp4" type="video/mp4">
      </video>
    </div>

    <div class="columns is-centered">
      Website template borrowed from&nbsp;<a href="https://nerfies.github.io/">NeRFies</a> 
    </div>
  </div>
</footer>


</body>
</html>
